# Addressing Cascading Failures
연쇄 실패란 양성 피드백에 의해서 시간이 지남에 따라 점점 커지는 실패다.
## Causes of Cascading Failures and Designing to Avoid Them
신중한 시스템 설계는 주요한 연쇄 실패를 처리하는 전형적인 몇가지 시나리오를 고려해야한다.  
### Server Overload
가장 흔한 연쇄 실패는 서버의 과부하이다. 특정 서버가 장애가 발생할 경우 다른 서버에 과부하가 발생하고,
로드밸런싱 컨트롤러에 의하 부하가 분산되면서 다른 서버에도 과부하를 일으키고 전체 서비스에 걸쳐서 장애가 발생 할 수 있다.
### Resource Exhaustion
서버에서 어떤 리소스가 고갈되는지에 따라서 나타나는 양상이 다를 수 있다.
- CPU: 일반적으로 리퀘스트의 처리가 늦어진다.in-flight requests 수 증가, 대기열의 수 증가등의 효과가 발생한다.
- Memory
- Threads
- File Descriptors: 파일, 네트워크등의 리소스를 표현하는 것으로 이것이 부족하면 네트워크 연결을 초기화 할 수 없게 된다.
### Dependencies among resources
리소스 고갈 시나리오는 서로 영향을 주고 받으며 발생한다.
예를 들어 FE의 GC 설정을 잘못하는 경우 캐시가 비효율적이게 되고, 이것이 BE의 과부하를 만들 수도 있는데, 
이렇듯 복잡하게 얽혀 있는 시나리오에서는 근본 원인을 찾아내는 게 쉽지 않다.
### Service Unavailability
리소스 고갈로 몇대의 서버가 과부하로 다운되면, 다른 서버가 요청을 처리하면서 과부화 된다.
다시 서버가 재시작 되자마자 많은 요청이 들어오고 바로 다시 다운되는 악순환이 발생할 수 있다.
로드밸런싱 레이어에서도 비슷한 상황이 발생할 수 있다. 
레임덕 상태등으로 에러를 되돌려주는 서버를 피하면서 부하를 분산하면, 정상작동하는 서버에 부하가 증가하고, 
이 서버또한 과부하상태가 되어서 정상적으로 동작하는 서버의 수가 점점 줄어든다.

## Preventing Server Overload
서버 과부하를 피하기 위한 전략
- 서버 용양 한계에 대한 부하 테스트 및 과부하에 대한 고장모드 테스트하기
- 품질이 저하된 결과를 제공하기
- 과부하시에 요청을 거부하는 서버 설치하기
- 서버를 과부하 상태에 빠지게 하기보다는 요청을 거부하는 높은 레벨의 시스템 설치하기
- 용량 계획세우기

### Queue Management
Queue 에 더이상 공간이 없을때 요청을 조기 거부 해야 한다.
thread 수 와 queue 길이의 비율을 2:1이 좋다.

### Load Shedding and Graceful Degradation
Load Shedding: 조기에 리퀘스트를 제한하기
Graceful degradation: 낮은 퀄리티의 응답을 돌려줌으로써 부하를 줄이기
degradation 모드는 잘 사용되지 않고, 운영경험이 적어서 위험도 높다. 서버의 작은 부분집합에서 이 모드를 실행시킴으로써 정기적으 연습해야 한다.
복잡한 shedding 과 Graceful Degradation 전략은 그 자체로 문제가 될 수 있다. 원하지 않을때 degraded mode 에 들어갈 수 있음으로 이것을 turnoff 하는 스위치를 준비해야 한다.

### Retries
프론트엔드에 의한 재시도의 증폭에 의해서 과부하가 발생할 수 있다.
재시도 요청에 의해서 전체 요청량이 증가하고 이로인해 과부하가 발생 할 수 있다.
과부하로 인한 장애가 발생했을 때 정상화를 위해서는 프론트엔드의 부하를 크게 줄이는 조치가 필요하다. 
재시도 스케줄링에 랜덤 익스포넨셜 백오프를 사용해야 한다. 재시도가 겹쳐서 다시 요청이 증폭될 수 있다.
재시도 횟수에 제한이 있어야 한다. 필요하다면 요청별로 재시도 횟수를 제한 할 게 아니라 글로벌하게 재시도 횟수를 제한해야 한다.
여러 층에 걸쳐서 재시도가 일어나면 거듭제곱으로 재시도 횟수가 늘어난다.
재시도 요청 가능 여부 등의 정보를 포함할 수 있는 통신을 해야한다.

### Latency and Deadlines
프론트엔드와 백엔드가 RPC 통신을 할 때, 프론트엔드는 응답을 기다리며 리소스를 소비한다.
RPC 데드라인은 프론트엔드가 포기하기 전까지 얼마나 요청을 기다릴 수 있는지 정의해서 백엔드가 프론트엔드의 리소스를 소비할 수 있는 시간을 제한한다. 

#### Picking a deadline
적절한 데드라인을 설정하는건 어렵다.
#### Missing deadlines
데드라인을 넘겨버리면 클라이언트가 요청을 포기하기 때문에 그 이후에 서버가 요청을 처리하면 그것은 의미 없는 일이 된다.
이를 방지하기 위해서는 서버는 요청에 대해서 계속해서 작업을 이어가기 전에 남은 시간을 확인해야 한다.
#### Deadline propagation
스택의 최상위에서 데드라인을 설정하고 데드라인이 전파되게 해야한다.
이를 통해서 이미 데드라인을 넘긴 요청에 대해서 하위 스택의 서버가 의미없이 요청을 처리하는 것을 방지할 수 있다.
#### Cancellation propagation
지연을 감소 시키기 위해서 다른 서버에 대비책 요청을 보내는 경우가 있다.
어떤 서버에서 응답을 받았다면, 나머지 서버에게 취소 요청을 해야 한다.
요청은 여러 서버로 팬아웃 하는 경우도 있기 때문에 취소 또한 전파되어야 한다.
#### Bimodal latency (이중으로 분포된 지연)
지연시간이 아주 긴 요청은 데드라인까지 스레드를 점유해서 스레드 자원의 부족을 야기하고 결과적으로 과부하를 야기할 수 있다.
예를 들어 1000 개의 thread 를 가진 FE 가 있고, 1000QPS 를 처리해야 할 때 쿼리당 처리시간이 100ms 인 경우, thread 는 100 개가 소모된다. 
이중 5% 가 완료되지 않는 요청이고 이것의 deadline이 100초 이면, 약 20초가 지나면 모든 thread 가 점유된다. 
이러한 문제를 발견하기 위해서는 리퀘스트별 평균 지연시간이 아닌, 지연시간 분포를 함께봐야 한다.
특정 백엔드의 오류 때문에 지연시간이 길어지는경우 요청이 처리되기를 기다리면서 스레드 리소스를 사용하기보다는 조기에 에러를 반환하도록 해야한다.
평균지연시간보다 몇배나 긴 데드라인을 설정하면 데드라인이 길수록 점유되는 스레드가 많아지고 결국 스레드 고갈을 야기한다.
어떤 클라이언트의 오작동으로 과부하가 발생 했을 때 그 클라이언트가 점유할 수 있는 스레드 수에 한계를 설정함으로 다른 클라이언트들이 영향을 받지 않게 할 수 있다.

## Slow Startup and Cold Caching
프로세스가 막 시작되었을 때는 다른 백엔드와의 커넥션을 처음 설정 하거나(생성된 커넥션은 재사용 된다.), 
Just-In-Time compilation 기술을 사용하는 JAVA처럼 런타임에 최적화가 이루어져서 성능이 떨어질 수 있다.
다음과 같은 이유로 캐시가 비어있을 때도 성능저하가 발생한다.
- 막 새 클러스터를 턴업 했을 때 
- 클러스터가 유지보수작업을 마치고 돌아왔을 때 
- 재시작했을 때
캐싱이 서비스에 상당한 영향이 있을 경우에는 다음과 같은 전략을 고려해 봐야 한다.
- 오버 프로비저닝
- 일반적인 연쇄 실패 방지 전략 사용, 특히 과부하 상태나 디그레이디드 모드일 때 요청을 조기에 거부하기.
- 클러스터에 부하를 추가할 때 천천히 부하를 증가시키는 방법, 캐시가 따듯해 진다.

### Always Go Downward in the Stack
* _롤아웃: 단계적 배포_
* _바이너리: 실행파일_
* _Drains: 비우기_
* _fail over: 대체 작동, 장애 조치_
* _autoscaling: 트래픽에 따른 자동 용량 확장_

프론트, 백엔드, 스토리의 스택이 있을때 스토리지의 문제는 프론트와 백엔드에 영향을 미치고 스토리지의 문제를 해결하면 모든 레이어의 문제가 해결된다.
백엔드가 서로 통신(intra-layer communication) 한다고 하면 문제가 더 복잡해 진다.
순환참조로 인한 데드락 문제가 발생, 보조 백엔드에 부하를 전달하면서 다시 오버해드가 발생, 전체 부하가 증가하는 문제,
intra-layer communication 에 사이클이 발생하는 문제, 백엔드가 다른 백엔드의 프록시로 동작하지 않게 프론트엔드가 다시 올바를 백엔드를 찾도록 해야 한다.

## Triggering Conditions for Cascading Failures
연쇄 실패를 야기할 수 있는 몇몇 요인을 알아본다.
### Process Death
몇몇 서버의 테스크들이 죽으면서 처리량을 감소 시킬 수 있다. 서버의 죽음을 야기하는 쿼리등 몇가지 이유가 원인일 수 있다.
### Process Updates
많은 수의 테스크가 동시 업데이트(바이너리 교체). 요청의 양과 사용 가능한 용량에 따라 업데이트 수를 조정하는 방법을 사용 하거나 푸시에 발생하는 오버헤드를 계산해서 푸쉬해야 한다.
### New Rollouts
시스템 변경의 배포. 연쇄 실패가 일어난다면 먼저 최근의 변경을 살펴봐야한다. 이런한 변경이 용량이나 요청의 특성에 영향을 미치는지 살펴봐야한다.  
### Organic Growth (유기적 성장)
특정 서비스의 변경보다, 사용량 증가에 따른 용량 조정이 이루어지지 않을 때 연쇄 실패가 일어난다.
### Planned Changes, Drains, or Turndowns
의존하고 있는 서비스의 클러스터가 드래인 상태가 되면 업스트립 서비스의 처리량이 감소하고,
멀리 떨어진 클러스터의 서비스에 의존하게 되면서 지연이 증가할 수 있다.
### Request profile changes
프론트엔드의 여러 설정 및 코드들이 변경되면서 요청의 특성이 변경되고 이것들이 원인이 될 수 있다.
### Resource limits
일부 클러스터는 리소스의 overcommitment 를 허용하는데 cpu 의 여유분은 언제는 사라질 수 있음으로 이에 의존하면 안된다.


## Testing for Cascading Failures
서비스가 연쇄실패에 취약한지를 발견 할 수 있는 테스팅 전략에 대해서 공부한다.
서비스가 다양한 환경에서 연쇄 실패에 들어가지 않는다는 확신을 얻기 위해서 부하가 많이 걸렸을 때 서비스가 어떻게 행동하는지 알아내기 위해 테스트를 해야 한다.

### Test Until Failure and Beyond
과부하 상태에서 서비스가 어떻게 동작하는지 확인하는 것이 연쇄 실패를 방지하는 첫 단계이다.
이를 파악하면 어떤 것들을 고쳐 나가야 하는지 알 수 있다. 또 실제 장애가 발생했을 때 디버깅 프로세스의 첫 단추를 찾을 수 있다.

컴퍼넌트가 실패할 때까지 테스트 해야 한다. 연쇄 실패에 취약한 컴퍼넌트는 장애가 발생할 것이다.
대비가 잘 되어있는 컴퍼넌트는 부하가 발생했을 때 적절히 요청을 거부하면서, 처리량 감소는 크지 않아야 한다.
이러한 테스트를 통해서 capacity planing의 기본이 되는 임계점을 파악할 수도 있다.
캐시를 사용하는 서비스들도 있으므로 점진적 부하 패턴과, 임펄스 부하 패턴을 모두 테스트 해야 한다.
과부하가 발생했다가 정상 부하로 돌아왔을 때 컴퍼넌트가 어떻게 동작 하는지도 테스트 해야 한다.
여러 장치가 잘 설정되었다고 생각되면 프러덕션환경의 일부에서 장애 테스트를 해볼 수도 있다. 이때 장치가 동작하지 않으면 수동으로 장애에 대한 조치를 해야할 수 있음을 유념해야 한다.

### Test Popular Clients
서비스를 사용하는 큰 클라이언트들이 어떻게 동작하는지도 테스트해 봐야한다.
큐가 있는지, 지수 백오프를 사용하는지 같은 부하 상황에 동작하는 장치가 있는지 등, 클라이언트를 이해하는게 좋다.

### Test Noncritical Backends
중요하지 않은 백엔드를 테스트하고, 이 백엔드가 사용불가가 서비스의 중요 컴퍼넌트에 영향을 주지 않는다는 것을 확인해야 한다.
만약 프론트엔드가 중요한 백엔드와 중요하지 않은 백엔드의 처리를 수반하는 요청을 가질 때,
중요하지 않은 백엔드가 사용불가능 하거나, 응답을 하지 않을 때 프론트 엔드가 어떻게 동작하는지 테스트 해야한다.
중요하지 않은 백엔드에 장애가 있을 때, 프론트엔드가 요청을 거부하거나, 리소스를 많이 사용하거나 지연시간이 길어지면 안된다.



## Immediate Steps to Address Cascading Failures
서비스가 연쇄 실패를 경함할 때 상황을 해결하기 위해 몇가지 다른 전략을 사용 할 수 있다.

### Increase Resources
단순하게 테스크를 추가하는게 도움이 될 수도 있지만, 데스 스파이럴에 빠진 상태라면 리소스를 추가하는건 해결책이 되지 않을 수도 있다.

### Stop Health Check Failures/Deaths
task 의 health 를 확인해서 재시작 하는 시스템은, 이로 인해 서비스를 unhealthy하게 만들 수 있다.
예를 들어 테스크의 절반이 재시작중일 때, 남은 테스크는 과부하게 걸려  Health checking 에 실패하고 재시작 될 수 있다.
따라서 모든 테스트가 안정될 때까지 이러한 기능을 off 해야 할 수도 있다.
클러스터 헬스 체킹(바이너리가 잘 동작하는가?), 서비스 헬스체킹(요청에 응답할 수 있는가?)은 다르다. 서비스 헬스체킹은 LB와 관련있다.

### Restart Servers
자바 서버가 GC 데스 스파이럴에 있거나, 데드라인이 없는 in-flight 요청으로 리소스가 소비되고 있거나, 데드락 상태라면 서버를 재시작 하는게 도움이 될 수 있다.
서버를 재시작 하기전에 장애의 원인을 파악해야한다. 서버를 재시작 하는 동안에 부하가 이동한다면 다른 곳에서 과부하를 만들 수 있다.
또 원인이 콜드캐쉬 문제인 경우 이러한 조치는 연쇄 실패를 증폭 시킬 수도 있다.

### Drop Traffic
트래픽을 제한하는 건 다른 수단이 동작하지 않을때 사용할 수 있다. 다음과 같은 방법으로 서비스를 재시작 할 수 있다.
- 연쇄 실패를 발생시키는 조건을 용량추가 등으로 해결 한다.
- 부하를 충분히 줄인다.
- 다수의 서버가 healty 상태가 되는걸 기다린다.
- 점진적으로 부하를 늘린다.
이 전략은 캐시가 웜업되고 커넥션을 생성하는등의 과정을 허용한다.
이 전략을 사용하면 유저에게 형향이 감으로 중요하지 않은 트래픽을 먼저 감소시키는 장치가 준비되어 있다면 그것을 먼저 사용해야 한다.
이 전략은 근본적인 문제를 해결했을 때 연쇄 중단을 다시 야기시키지 않는다는 것을 명심해야한다.
트리거 조건의 해결, 또는 근본 원인의 제거를 하지 않으면 트래픽이 원래대로 돌아왔을 때 다시 연쇄 실패를 야기할 수 있다.

### Enter Degraded Modes
저하된 결과를 되돌려 주거나 중요하지 않은 트래픽을 드롭해서 성능저하를 방지한다.

### Eliminate Batch Load
장애동안 중요하지 않은 작업의 부하를 제거한다.

### Eliminate Bad Traffic
어떤 쿼리들은 많은 부하 또는 고장을 만든다. 이것들을 블라킹 하거나 제거 하는 것을 고려해야 한다.


## Cascading Failure and Shakespeare
일본에서 셰익스피어에 대한 다큐멘터리를 방송했고, 참고자료를 찾아볼 수 있는 곳으로 Shakespeare 서비스를 명시적으로 추천했다.
아시아 데이터 센터의 트래픽이 서비스의 용량을 초과해서 급증했다.

다행히 잠재적 실패 완화하는 몇가지 안전장치가 준비되어 있었다.Production Readiness Review process 를 통해 이미 팀은 몇가지 문제를 해결했다.
예를 들면 개발자들은  graceful degradation 기능을 서비스에 구현했다.
용량이 부족해졌을 때, 서버는 사진을 되돌려주지 않거나, 지수 백오프 등의 동작을 했다.
이러한 안전장치에도 불구하고 테스크수가 줄어들었다.

그 결과 SRE가 호출되었다. SRE는 일시적으로 아시아 데이터 센터의 용량을 증가시켰고 서비스는 복구되었다.
이 시나리오의 재발 방지를 위한 사후검토가 이루어졌고, 몇가지 액션 아이템이 도출되었다.
예를 들어 과부하시에 LB가 부하를 다른 근처 데이터센터로 옮기도록 하는 것 이었다.
또한 오토스케일링 기능을 켜서 트래픽이 증가하면 자동으로 테스트수가 증가하도록 함으로써 같은 형태의 이슈에 대해서 걱정하지 않게 되었다.


## Closing Remarks
서비스가 한계점(breaking point)를 넘어가면, 모든 요청을 완전하게 처리하기보다는 에러를 되돌려 주거나 낮은 품질의 결과를 돌려주는게 낫다.
연쇄 실패를 방지하기 위해서는 이러한 한계점이 어디에 있는지, 이것을 넘어서면 서비스가 어떻게 동작하는지 이해 하는것이 중요하다.

정상 상태를 개선하기 위한 시스템 변경은 대규모 실패를 야기할 위험이 있다.
장애 발생시 재시도, 부하 옮기기, 캐시 추가등의 개선을 시행하면 이것들로 인해서 대규모 장애가 발생할 수 있음으로,
변경사항을 평가할 때 이 개선이 다른 장애를 야기하는 트레이드오프가 되지 않도록 주의해야 한다.
